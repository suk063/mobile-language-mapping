#!/usr/bin/env python3
"""
RGB-D Data Explorer with Open3D Point Cloud Visualization

This tool loads RGB-D data generated by generate_rgbd_data.py and visualizes it as
point clouds using Open3D. Each trajectory is displayed as a complete point cloud
combining all timesteps.

Usage:
    python rgbd_data_explorer.py <data_file> [--sensor <sensor_name>] [--max_depth <value>] [--point-size <size>] [--downsample <factor>]

Controls:
    - Left/Right Arrow: Navigate between trajectories
    - R: Reset view
    - Q/Esc: Quit
"""

import argparse
import os
import sys
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import open3d as o3d
import torch
from tqdm import tqdm

from utils import depth_to_positions


class RGBDDataExplorer:
    def __init__(
        self,
        data_file: str,
        sensor_names: list[str],
        min_depth: float = 0.0,
        max_depth: float = 5.0,
        trajectory_downsample_factor: int = 1,
        image_downsample_factor: int = 1,
    ):
        """
        Initialize the RGB-D Data Explorer.

        Args:
            data_file: Path to the RGB-D data file (.pt)
            sensor_names: List of sensor names to visualize (e.g., 'fetch_head', 'fetch_hand')
            min_depth: Minimum depth value for filtering points
            max_depth: Maximum depth value for filtering points
            point_size: Size of points in the visualization
            trajectory_downsample_factor: Factor to downsample trajectories (1 = no downsampling, 2 = every 2nd waypoint, etc.)
            image_downsample_factor: Factor to downsample images (1 = no downsampling, 2 = every 2nd image, etc.)
        """
        self.data_file = data_file
        self.sensor_names = sensor_names
        self.min_depth = max(min_depth, 0.0)
        self.max_depth = max_depth if max_depth > 0 else float("inf")
        self.trajectory_downsample_factor = trajectory_downsample_factor
        self.image_downsample_factor = image_downsample_factor
        self.current_traj = 0
        self.playing = False

        # Load data
        assert os.path.exists(self.data_file), f"Data file '{self.data_file}' does not exist"
        assert self.data_file.endswith(".pt"), "Data file must be a .pt file"
        self.data = torch.load(self.data_file, mmap=True)
        self.episode_configs = self.data["episode_configs"]
        self.episode_configs = {f"traj_{i}": episode_config for i, episode_config in enumerate(self.episode_configs)}
        self.trajectory_keys = sorted([k for k in self.data.keys() if k.startswith("traj_")])
        self.num_trajectories = len(self.trajectory_keys)

        if self.num_trajectories == 0:
            raise ValueError("No trajectory data found in the file")

        # Get intrinsic matrix
        self.intrinsic = self.data["intrinsic"]

        # Initialize Open3D
        self.vis = o3d.visualization.VisualizerWithKeyCallback()
        # Register key callbacks
        self._register_key_callbacks()

        self.vis.create_window(window_name="RGB-D Data Explorer", width=1280, height=720)

        # Initialize point cloud
        self.pcd = o3d.geometry.PointCloud()
        self._update_pcd()
        self.vis.add_geometry(self.pcd)

        # Add coordinate frame
        self.coord_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.5)
        self.vis.add_geometry(self.coord_frame)

    def _register_key_callbacks(self):
        """Register keyboard callbacks for navigation."""
        self.vis.register_key_action_callback(262, self._next_trajectory)  # Right
        self.vis.register_key_action_callback(263, self._prev_trajectory)  # Left

    def _next_trajectory(self, vis, action, mod):
        """Navigate to the next trajectory."""
        if action != 0:  # Only handle key press events
            return False
        if self.current_traj < self.num_trajectories - 1:
            self.current_traj += 1
            self._update_display()
            print(f"Trajectory: {self.current_traj + 1}/{self.num_trajectories}")
            return True
        else:
            print("Already at the last trajectory")
            return False

    def _prev_trajectory(self, vis, action, mod):
        """Navigate to the previous trajectory."""
        if action != 0:  # Only handle key press events
            return False
        if self.current_traj > 0:
            self.current_traj -= 1
            self._update_display()
            print(f"Trajectory: {self.current_traj + 1}/{self.num_trajectories}")
            return True
        else:
            print("Already at the first trajectory")
            return False

    def _depth_to_point_cloud_trajectory(self, traj_data: Dict) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Convert entire trajectory RGB-D data to point cloud.

        Args:
            traj_data: Dictionary containing trajectory sensor data

        Returns:
            points: 3D points (N, 3) for entire trajectory
            colors: RGB colors (N, 3) for entire trajectory
        """
        all_points = []
        all_colors = []

        rgb_data = traj_data["rgb"]  # (T, H, W, 3)
        depth_data = traj_data["depth"]  # (T, H, W)
        extrinsic_data = traj_data["extrinsic"]  # (T, 3, 4) or (T, 4, 4)

        num_timesteps = len(rgb_data)
        for t in tqdm(range(0, num_timesteps, self.trajectory_downsample_factor), desc="Sensor Traj.", ncols=80):
            rgb = rgb_data[t]  # (H, W, 3)
            depth = depth_data[t]  # (H, W)
            extrinsic = extrinsic_data[t]  # (3, 4) or (4, 4)

            # Convert to torch tensors if needed
            if not isinstance(rgb, torch.Tensor):
                rgb = torch.tensor(rgb, dtype=torch.float32)
            if not isinstance(depth, torch.Tensor):
                depth = torch.tensor(depth, dtype=torch.float32)
            if not isinstance(extrinsic, torch.Tensor):
                extrinsic = torch.tensor(extrinsic, dtype=torch.float32)

            # Convert frame to point cloud
            points, colors = self._depth_to_point_cloud(rgb, depth, extrinsic)

            if len(points) > 0:
                all_points.append(points)
                all_colors.append(colors)

        if all_points:
            # Concatenate all points and colors
            combined_points = torch.cat(all_points, dim=0)
            combined_colors = torch.cat(all_colors, dim=0)
            return combined_points, combined_colors
        else:
            return torch.zeros((0, 3)), torch.zeros((0, 3))

    def _depth_to_point_cloud(
        self, rgb: torch.Tensor, depth: torch.Tensor, extrinsic: torch.Tensor
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Convert RGB-D data to point cloud using depth_to_positions.

        Args:
            rgb: RGB image tensor (H, W, 3)
            depth: Depth image tensor (H, W)
            extrinsic: Camera extrinsic matrix (3, 4) or (4, 4)

        Returns:
            points: 3D points (N, 3)
            colors: RGB colors (N, 3)
        """
        # Ensure inputs are proper tensors
        rgb = rgb.float()
        depth = depth.float() / 1000.0  # convert from mm to meters
        extrinsic = extrinsic.float()

        # Use depth_to_positions to get world coordinates
        world_coords = depth_to_positions(depth, self.intrinsic, extrinsic)  # (H, W, 3)

        # Filter points by depth
        valid_mask = (depth > self.min_depth) & (depth < self.max_depth)
        points = world_coords[valid_mask]  # (N, 3)

        # Get corresponding colors
        colors = rgb[valid_mask] / 255.0  # (N, 3)

        return points, colors

    def _update_pcd(self):
        """Update the point cloud display with entire trajectory."""
        traj_key = self.trajectory_keys[self.current_traj]
        traj_data = self.data[traj_key]

        episode_config = self.episode_configs[traj_key]
        print(f"Trajectory: {traj_key}, episode config: {episode_config}")

        # Convert the entire trajectory to point cloud
        points = []
        colors = []
        for sensor in tqdm(self.sensor_names, desc="Sensors", ncols=80):
            points_sensor, colors_sensor = self._depth_to_point_cloud_trajectory(traj_data[sensor])
            points.append(points_sensor)
            colors.append(colors_sensor)
        points = torch.cat(points, dim=0)
        colors = torch.cat(colors, dim=0)

        # Apply downsampling if requested
        if self.image_downsample_factor > 1:
            indices = torch.arange(0, len(points), self.image_downsample_factor)
            points = points[indices]
            colors = colors[indices]
            print(f"Downsampled to {len(points)} points (factor: {self.image_downsample_factor})")

        print(f"Generated {len(points)} total points for the trajectory")

        # Update Open3D point cloud
        self.pcd.points = o3d.utility.Vector3dVector(points.double().numpy())
        self.pcd.colors = o3d.utility.Vector3dVector(colors.double().numpy())

    def _update_display(self):
        self._update_pcd()
        self.vis.update_geometry(self.pcd)
        self.vis.poll_events()
        self.vis.update_renderer()

    def run(self):
        """Run the visualization loop."""
        print("\n" + "=" * 60)
        print("RGB-D Data Explorer Controls:")
        print("  Left/Right Arrow: Navigate between trajectories")
        print("=" * 60)

        self.vis.run()
        self.vis.destroy_window()


def main():
    parser = argparse.ArgumentParser(description="Visualize RGB-D data as point clouds with Open3D")
    parser.add_argument("--data-file", help="Path to RGB-D data file (.pt)")
    parser.add_argument(
        "--sensor-names",
        nargs="+",
        required=True,
        help="List of sensor names to visualize (e.g., 'fetch_head', 'fetch_hand')",
    )
    parser.add_argument(
        "--min-depth", type=float, default=0.0, help="Minimum depth value for filtering points (default: 0.0)"
    )
    parser.add_argument(
        "--max-depth", type=float, default=50.0, help="Maximum depth value for filtering points (default: 50.0)"
    )
    parser.add_argument(
        "--trajectory-downsample",
        type=int,
        default=1,
        help="Downsample factor for trajectories (default: 1, no downsampling)",
    )
    parser.add_argument(
        "--image-downsample", type=int, default=1, help="Downsample factor for images (default: 1, no downsampling)"
    )

    args = parser.parse_args()

    if not os.path.exists(args.data_file):
        print(f"Error: File '{args.data_file}' not found")
        sys.exit(1)

    explorer = RGBDDataExplorer(
        args.data_file,
        args.sensor_names,
        args.min_depth,
        args.max_depth,
        args.trajectory_downsample,
        args.image_downsample,
    )
    explorer.run()


if __name__ == "__main__":
    main()
